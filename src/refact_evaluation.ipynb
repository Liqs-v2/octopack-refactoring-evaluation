{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYu/gjrBNUxdXuyEdhLJsP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Liqs-v2/octopack-refactoring-evaluation/blob/main/src/refact_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "YgY_RTHvOlJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r70tcVAdP-wo",
        "outputId": "567d3621-5d00-4a91-ec1f-758358939920"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VyQeyOTBNlvH"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "Gk1FiSsZPQi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"smallcloudai/Refact-1_6B-fim\"\n",
        "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage"
      ],
      "metadata": {
        "id": "mvC1mFPJPJr1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)"
      ],
      "metadata": {
        "id": "URTogFgQPLNd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "PYZshSb9REQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"bigcode/humanevalpack\", \"python\")"
      ],
      "metadata": {
        "id": "hfZ2n9DURFo2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model"
      ],
      "metadata": {
        "id": "YoaTp_MAbP7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build queries for prompts"
      ],
      "metadata": {
        "id": "RybOFzjWbS3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a query format matching the one specified in appendix N of the [octopack paper](https://arxiv.org/abs/2308.07124) for `HumanEvalFix`. In Table 16 of appendix N the authors state that \"If no function start or no context is present,\n",
        "that part is not added to the prompt (and the preceding newline is also removed).\" I interpreted the horizontal bars in their sample query for `HumanEvalFix` as newlines, thus adding \"\\n\\n\" between the \"Instruction\" and \"Context sections as well as the \"Context\" and \"Function start\" sections."
      ],
      "metadata": {
        "id": "itFsRHbpbVpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_query = dataset['test'][0]['instruction'] + \"\\n\\n\" + dataset['test'][0]['declaration'] + dataset['test'][0]['buggy_solution'] + \"\\n\" + dataset['test'][0]['declaration']"
      ],
      "metadata": {
        "id": "s5ANfPHfYuuf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiwJQHw9ZYyM",
        "outputId": "d08a5988-1ae4-4383-a4c4-e3142879f76b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a Python function `has_close_elements(numbers: List[float], threshold: float) -> bool` to solve the following problem:\n",
            "Check if in given list of numbers, are any two numbers closer to each other than\n",
            "given threshold.\n",
            ">>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "False\n",
            ">>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "True\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    for idx, elem in enumerate(numbers):\n",
            "        for idx2, elem2 in enumerate(numbers):\n",
            "            if idx != idx2:\n",
            "                distance = elem - elem2\n",
            "                if distance < threshold:\n",
            "                    return True\n",
            "\n",
            "    return False\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt model for refactoring"
      ],
      "metadata": {
        "id": "EqhcevwNPTHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the authors in the [octopack paper](https://arxiv.org/abs/2308.07124) suggest (see appendix N) I \"do not optimize prompts and go with the format provided by the respective\n",
        "model authors or the most intuitive format if none is provided.\"\n",
        "\n",
        "Since the [Refact-1.6B model](https://huggingface.co/smallcloudai/Refact-1_6B-fim) provides a prompting template for chat use, which is suggested in the task description, we use this template."
      ],
      "metadata": {
        "id": "jVBjmcGIXh44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"<empty_output>SYSTEM {system}\\n\" \\\n",
        "                  \"<empty_output>USER {query}\\n\" \\\n",
        "                  \"<empty_output>ASSISTANT\""
      ],
      "metadata": {
        "id": "NtFPdBlsdDgr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model evaluation utilities\n",
        "To evaluate our model we perform pairwise comparison between the lines returned by the model and the solution specified in the dataset. To be a bit more lenient, we remove irrelevant lines that just contain whitespaces from both solutions."
      ],
      "metadata": {
        "id": "HAhWv4cWlt7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_non_empty_lines_from(solution: List[str]) -> List[str]:\n",
        "  \"\"\"\n",
        "  Sp\n",
        "  \"\"\"\n",
        "  solution_lines = solution.split('\\n')\n",
        "  return [line for line in solution_lines if line.strip() != '']\n",
        "\n",
        "def is_model_solution_correct(model_solution, sample_solution) -> bool:\n",
        "  model_solution_lines = extract_non_empty_lines_from(model_solution)\n",
        "  sample_solution_lines = extract_non_empty_lines_from(sample_solution)\n",
        "\n",
        "  if len(sample_solution_lines) != len(model_solution_lines):\n",
        "    return False\n",
        "\n",
        "  solution_lines = zip(sample_solution_lines, model_solution_lines)\n",
        "\n",
        "  for sample_solution_line, model_solution_line in solution_lines:\n",
        "    if sample_solution_line != model_solution_line:\n",
        "      print(\"Solution mismatch detected!\")\n",
        "      print(f\"Sample solution: {sample_solution_line}\")\n",
        "      print(f\"Model solution: {model_solution_line}\")\n",
        "      return False"
      ],
      "metadata": {
        "id": "3cGMcomHelII"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in dataset['test']:\n",
        "  instruction = data['instruction']\n",
        "  context = data['declaration'] + data['buggy_solution']\n",
        "  function_start = data['declaration']\n",
        "  query = instruction + \"\\n\\n\" + context + \"\\n\" + function_start\n",
        "\n",
        "  prompt = prompt_template.format(system=\"You are a programming assistant\",\n",
        "                                query=query)\n",
        "\n",
        "  inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "  outputs = model.generate(inputs, max_length=5000, temperature=0.2)\n",
        "  result = tokenizer.decode(outputs[0])\n",
        "\n",
        "  model_solution = (result.split(\"ASSISTANT \")[-1]).split(\"<empty_output>\")[0]\n",
        "  sample_solution = data['declaration'] + data['canonical_solution']\n",
        "\n",
        "  # Persist model solution result\n",
        "  data['is_model_solution_correct'] = is_model_solution_correct(model_solution, sample_solution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IskObBe2dFj1",
        "outputId": "af9e2749-7d15-4f84-9714-1ddc758a5f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution mismatch detected!\n",
            "Sample solution:             if idx != idx2:\n",
            "Model solution:             if idx!= idx2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution mismatch detected!\n",
            "Sample solution:             if current_depth == 0:\n",
            "Model solution:             if current_depth < 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution mismatch detected!\n",
            "Sample solution:     return number % 1.0\n",
            "Model solution:     return number % 1.0 + 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution mismatch detected!\n",
            "Sample solution:     return sum(abs(x - mean) for x in numbers) / len(numbers)\n",
            "Model solution:     return sum(abs(x - mean) for x in numbers) / mean\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution mismatch detected!\n",
            "Sample solution:                 depth -= 1\n",
            "Model solution:                 max_depth -= 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution mismatch detected!\n",
            "Sample solution:     while not is_palindrome(string[beginning_of_suffix:]):\n",
            "Model solution:     while not is_palindrome(string):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution mismatch detected!\n",
            "Sample solution:             return '0'\n",
            "Model solution:             return '1'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution mismatch detected!\n",
            "Sample solution:     return a\n",
            "Model solution:     return b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution mismatch detected!\n",
            "Sample solution:     for i in range(len(string)):\n",
            "Model solution:     for i in range(len(string)-1):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution mismatch detected!\n",
            "Sample solution:     return ' '.join([str(x) for x in range(n + 1)])\n",
            "Model solution:     return''.join([str(x) for x in range(n)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution mismatch detected!\n",
            "Sample solution:     note_map = {'o': 4, 'o|': 2, '.|': 1}\n",
            "Model solution:     note_map = {'o': 3, 'o|': 2, '.|': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bgO_d9QPmacg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}